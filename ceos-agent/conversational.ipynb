{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "https://python.langchain.com/docs/expression_language/cookbook/retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "debug= False\n",
    "set_verbose(debug)\n",
    "set_debug(debug)\n",
    "\n",
    "OpenAPIKey = os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Test for a Simple Prompt and Feed to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='tell me a joke about bears')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"tell me a joke about {foo}\")\n",
    "model = ChatOpenAI(api_key=OpenAPIKey)\n",
    "\n",
    "conversation_chain = prompt | model\n",
    "\n",
    "conversation_chain.invoke({\"foo\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Test to see How ChatPromptTemplate works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob. How can I assist you today?')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "                (\"human\", \"Hello, how are you doing?\"),\n",
    "                (\"ai\", \"I'm doing well, thanks!\"),\n",
    "                (\"human\", \"{user_input}\"),\n",
    "            ])\n",
    "\n",
    "conversation_chain = prompt | model\n",
    "\n",
    "conversation_chain.invoke({\n",
    "    \"name\": \"Bob\",\n",
    "    \"user_input\": \"What is your name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chat Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Given the following conversation and a follow up question, rephrase the follow up \n",
    "question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "------------------------------\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "# Answer Prompt is the one that we expect the assistant to provide an answer in\n",
    "answer_prompt = ChatPromptTemplate.from_template(\"\"\"{context}\n",
    "\n",
    "                                          \n",
    "-----------\n",
    "Answer the question below based only on the above context (without mention the context in \n",
    "the response).\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mod': 'What is your name?'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"Human: \" + dialogue_turn[0]\n",
    "        ai = \"Assistant: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "test = RunnableParallel(    \n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    question=lambda x: x[\"question\"],\n",
    ") | {\"mod\": lambda x: x[\"question\"]}\n",
    "\n",
    "test.invoke({\n",
    "    \"chat_history\": [(\"Hello\", \"Hi\"), (\"How are you?\", \"I'm fine\")],\n",
    "    \"question\": \"What is your name?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Conversational Chain & Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': AIMessage(content='What is the context of the meaning of life, considering the vastness of the universe?')}\n",
      "What is the context of the meaning of life, considering the vastness of the universe?\n",
      "\n",
      "--------------------------------------------------\n",
      "Tokens Used: 85\n",
      "\tPrompt Tokens: 67\n",
      "\tCompletion Tokens: 18\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0001365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    api_key=OpenAPIKey,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "test = RunnableParallel(    \n",
    "    chat_history=lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    question=RunnablePassthrough(),\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = conversation_chain.invoke({\n",
    "    \"chat_history\": [],\n",
    "    \"question\": \"Given that the universe is vast, I struggle to grasp the context of the meaning of life?\"\n",
    "    })\n",
    "\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"-\"* 50)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use In-Memory Embeddings\n",
    "\n",
    "If using FAISS\n",
    "1. ```import sys```\n",
    "2. ```!{sys.executable} -m pip install faiss-cpu```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='How do I install the indoor climate sensors?  \\nIt is important that you install the indoor sensors in the right place. The sensors should be placed in the middle of the room, at a height of 1.5 meters. The sensors should not be placed in direct sunlight or near heat sources such as radiators or lamps. Do not place any sensors near windows or doors.  \\nPlace one sensor at the north wall and one at the south wall to be able to measure the temperature difference between the two sensors.' metadata={'Header 1': 'Crossbreed Smarter Heating', 'Header 2': 'Installation and Positioning of Indoor Climate Sensors'}\n",
      "page_content='How do I install the indoor climate sensors?  \\nIt is important that you install the indoor sensors in the right place. The sensors should be placed in the middle of the room, at a height of 1.5 meters. The sensors should not be placed in direct sunlight or near heat sources such as radiators or lamps. Do not place any sensors near windows or doors.  \\nPlace one sensor at the north wall and one at the south wall to be able to measure the temperature difference between the two sensors.' metadata={'Header 1': 'Crossbreed Smarter Heating', 'Header 2': 'Installation and Positioning of Indoor Climate Sensors'}\n",
      "page_content='The indoor climate sensors are from Sensitive, what do I need to provide?  \\nYou need to provide with the following information:  \\n* The sensor ID, for example: \"S14\".\\n* The Tenant ID, for example \"myAccount\".' metadata={'Header 1': 'Crossbreed Smarter Heating', 'Header 2': 'Indoor Sensors from Sensative'}\n",
      "page_content='The indoor climate sensors are from Sensitive, what do I need to provide?  \\nYou need to provide with the following information:  \\n* The sensor ID, for example: \"S14\".\\n* The Tenant ID, for example \"myAccount\".' metadata={'Header 1': 'Crossbreed Smarter Heating', 'Header 2': 'Indoor Sensors from Sensative'}\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('./data/training', glob=\"**/*.md\", loader_cls=TextLoader) # TextLoader == raw text\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"Header 1\"), # Head level, metadata key\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "])\n",
    "\n",
    "splitted_docs: List[Document] = []\n",
    "\n",
    "for doc in docs:     \n",
    "  splitted_docs.extend(splitter.split_text(doc.page_content))\n",
    "\n",
    "   \n",
    "chroma = Chroma.from_documents(\n",
    "  documents=splitted_docs,\n",
    "  embedding=OpenAIEmbeddings(api_key=OpenAPIKey)\n",
    ")\n",
    "\n",
    "result = chroma.similarity_search(\"How do I install indoor sensors?\")\n",
    "\n",
    "for res in result:\n",
    "  print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Question & Answer Chain with Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process for installing indoor sensors is to place them in the middle of the room\n",
      " at a height of 1.5 meters. They should not be placed in direct sunlight or near heat\n",
      " sources, windows, or doors. Additionally, one sensor should be placed at the north\n",
      " wall and one at the south wall to measure the temperature difference between them\n",
      ".\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "answerChain = conversation_chain | {\n",
    "  \"question\": RunnablePassthrough(),\n",
    "  \"context\": itemgetter(\"question\") | chroma.as_retriever() | format_docs,\n",
    "} | answer_prompt | model | StrOutputParser()\n",
    "\n",
    "cnt = 0\n",
    "max_len = 80\n",
    "\n",
    "for chunk in answerChain.stream({\n",
    "    \"chat_history\": [],\n",
    "    \"question\": \"How do I install indoor sensors?\"\n",
    "}): \n",
    "  if cnt > max_len:\n",
    "    print(\"\\n\", end=\"\")\n",
    "    cnt = 0\n",
    "    \n",
    "  cnt += len(chunk)\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma.as_retriever(), llm=model\n",
    ")\n",
    "\n",
    "unique_docs = retriever.get_relevant_documents(query=\"How do I install indoor sensors for best results?\")\n",
    "for doc in unique_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parent Document Retriever / MultiVector Retriever\n",
    "\n",
    "TODO: Do some testing with this\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
